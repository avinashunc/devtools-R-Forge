{"nbformat_minor": 0, "cells": [{"source": "# R-Forge:", "cell_type": "markdown", "metadata": {}}, {"source": "Le but c'est de r\u00e9cup\u00e9rer la liste des paquets de R-Forge, ci dessous la proc\u00e9dure:", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "\nfrom bs4 import BeautifulSoup\nimport urllib2\n#file = open('repoforge.csv', 'w')\nfor i in range(1, 95):\n    url = urllib2.urlopen('https://r-forge.r-project.org/softwaremap/full_list.php?page='+str(i))\n    html = url.read()\n    soup = BeautifulSoup(html)\n    for k in soup.find_all('span', property=\"doap:name\"):\n        nom = k.string      #ici on a les nom des packages\n        lien = k.find_parent(\"a\")\n        hflien = lien.get(\"href\")\n    #file.write(nom.encode('utf-8') +\",\"+ hflien.encode('utf-8') +\"\\n\")", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Jusqu'ici on r\u00e9cup\u00e8re les noms des paquets avec les liens, il s'agit de 1880 paquets (pour les avoir dans un fichier csv il suffit de decommenter la 3eme et la derni\u00e8re ligne). Ensuite pour r\u00e9cup\u00e9rer le description file il faut extraire le 'vrai nom' depuis le liens d'avant (hflien) et le concatener \u00e0 chaque fois avec le lien ou y a le descriptin file. les probl\u00e8mes rencontr\u00e9s ici c'est que dans le cas ou le paquet ne poss\u00e8de pas de description file on a un contenu html ou c'est not\u00e9 erreur 404 sinon dans le cas ou le description file existe on a de text. Donc il faut filtrer le contenu.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import  csv\nimport requests\n\n#des = open('DfRf.txt', 'w')\nfile = csv.reader(open('repoforge.csv', 'rb'))   #je r\u00e9cup\u00e8re mon fichier csv (nom,lien)\n\nn=m=l=0\n\nfor row in file:\n    if len(row)>0:\n        url = row[1]\n        x = url.rsplit(\"/\", 2)\n        nom = x[1]    #je r\u00e9cup\u00e8re le 'vrai nom du paquet'\n        content = requests.get(\"https://r-forge.r-project.org/scm/viewvc.php/*checkout*/pkg/DESCRIPTION?root=\" + nom)\n        if content.content.startswith('\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"'):\n            n+=1\n        else:\n            if content.content.startswith('\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>'):\n                m+=1\n            else:\n                #print >> des, content.content", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "On utiliser une autre approche pour avoir le description file avec un historique aussi c'est d'utiliser BeatifulSoup on suivant le lien du d\u00e9part puis les liens qui m\u00e8nent vers tous les descriptions files (pas seulement de la derni\u00e8re version) cette proc\u00e8dure est couteuse au niveau temporelle (ci dessous le code sans prendre en compte le fichier ou on a nom et le lien du paquet)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from bs4 import BeautifulSoup\nimport urllib2\n\n#file = open('extractALLDF.txt', 'w')\nfor i in range(1, 95):\n    url = urllib2.urlopen('https://r-forge.r-project.org/softwaremap/full_list.php?page='+str(i))\n    html = url.read()\n    soup = BeautifulSoup(html)\n    for k in soup.find_all('span', property=\"doap:name\"):\n        nom = k.string      #ici on a les nom des packages\n        lien = k.find_parent(\"a\")\n        hflien = lien.get(\"href\")\n        rep = hflien.rsplit(\"/\", 2)\n        repo = rep[1]\n        sitePack =[\"https://r-forge.r-project.org/scm/viewvc.php/pkg/?root=\", repo]\n        urlpack = \"\".join(sitePack)  #### jusque ici j'ai les lien vers les pkg ou y a DF\n        lec = urllib2.urlopen(urlpack)\n        res = lec.read()\n        sp = BeautifulSoup(res)\n        for item in sp.find_all('a', attrs={'name':'DESCRIPTION'}):\n            lDesc = item.get(\"href\")\n            compDesc = [\"https://r-forge.r-project.org\", lDesc]\n            compDesc2 = \"\".join(compDesc)\n            dwnlec=urllib2.urlopen(compDesc2)\n            rdDwnLec = dwnlec.read()\n            sDown = BeautifulSoup(rdDwnLec)\n            for d in sDown.find_all('a', text=\"download\"):\n                hrefDown = d.get(\"href\")\n                joinlinkDown = [\"https://r-forge.r-project.org\", hrefDown]\n                linkDown = \"\".join(joinlinkDown)\n                opLinkDown = urllib2.urlopen(linkDown)\n                rdLinkDown = opLinkDown.read()\n                print(rdLinkDown)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}