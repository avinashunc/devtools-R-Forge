{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-Forge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but c'est de récupérer la liste des paquets de R-Forge, ci dessous la procédure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "#file = open('repoforge.csv', 'w')\n",
    "for i in range(1, 95):\n",
    "    url = urllib2.urlopen('https://r-forge.r-project.org/softwaremap/full_list.php?page='+str(i))\n",
    "    html = url.read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    for k in soup.find_all('span', property=\"doap:name\"):\n",
    "        nom = k.string      #ici on a les nom des packages\n",
    "        lien = k.find_parent(\"a\")\n",
    "        hflien = lien.get(\"href\")\n",
    "    #file.write(nom.encode('utf-8') +\",\"+ hflien.encode('utf-8') +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jusqu'ici on récupère les noms des paquets avec les liens, il s'agit de 1880 paquets (pour les avoir dans un fichier csv il suffit de decommenter la 3eme et la dernière ligne). Ensuite pour récupérer le description file il faut extraire le 'vrai nom' depuis le liens d'avant (hflien) et le concatener à chaque fois avec le lien ou y a le descriptin file. les problèmes rencontrés ici c'est que dans le cas ou le paquet ne possède pas de description file on a un contenu html ou c'est noté erreur 404 sinon dans le cas ou le description file existe on a de text. Donc il faut filtrer le contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  csv\n",
    "import requests\n",
    "\n",
    "#des = open('DfRf.txt', 'w')\n",
    "file = csv.reader(open('repoforge.csv', 'rb'))   #je récupère mon fichier csv (nom,lien)\n",
    "\n",
    "n=m=l=0\n",
    "\n",
    "for row in file:\n",
    "    if len(row)>0:\n",
    "        url = row[1]\n",
    "        x = url.rsplit(\"/\", 2)\n",
    "        nom = x[1]    #je récupère le 'vrai nom du paquet'\n",
    "        content = requests.get(\"https://r-forge.r-project.org/scm/viewvc.php/*checkout*/pkg/DESCRIPTION?root=\" + nom)\n",
    "        if content.content.startswith('\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"'):\n",
    "            n+=1\n",
    "        else:\n",
    "            if content.content.startswith('\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>'):\n",
    "                m+=1\n",
    "            else:\n",
    "                #print >> des, content.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utiliser une autre approche pour avoir le description file avec un historique aussi c'est d'utiliser BeatifulSoup on suivant le lien du départ puis les liens qui mènent vers tous les descriptions files (pas seulement de la dernière version) cette procèdure est couteuse au niveau temporelle (ci dessous le code sans prendre en compte le fichier ou on a nom et le lien du paquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "\n",
    "#file = open('extractALLDF.txt', 'w')\n",
    "for i in range(1, 95):\n",
    "    url = urllib2.urlopen('https://r-forge.r-project.org/softwaremap/full_list.php?page='+str(i))\n",
    "    html = url.read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    for k in soup.find_all('span', property=\"doap:name\"):\n",
    "        nom = k.string      #ici on a les nom des packages\n",
    "        lien = k.find_parent(\"a\")\n",
    "        hflien = lien.get(\"href\")\n",
    "        rep = hflien.rsplit(\"/\", 2)\n",
    "        repo = rep[1]\n",
    "        sitePack =[\"https://r-forge.r-project.org/scm/viewvc.php/pkg/?root=\", repo]\n",
    "        urlpack = \"\".join(sitePack)  #### jusque ici j'ai les lien vers les pkg ou y a DF\n",
    "        lec = urllib2.urlopen(urlpack)\n",
    "        res = lec.read()\n",
    "        sp = BeautifulSoup(res)\n",
    "        for item in sp.find_all('a', attrs={'name':'DESCRIPTION'}):\n",
    "            lDesc = item.get(\"href\")\n",
    "            compDesc = [\"https://r-forge.r-project.org\", lDesc]\n",
    "            compDesc2 = \"\".join(compDesc)\n",
    "            dwnlec=urllib2.urlopen(compDesc2)\n",
    "            rdDwnLec = dwnlec.read()\n",
    "            sDown = BeautifulSoup(rdDwnLec)\n",
    "            for d in sDown.find_all('a', text=\"download\"):\n",
    "                hrefDown = d.get(\"href\")\n",
    "                joinlinkDown = [\"https://r-forge.r-project.org\", hrefDown]\n",
    "                linkDown = \"\".join(joinlinkDown)\n",
    "                opLinkDown = urllib2.urlopen(linkDown)\n",
    "                rdLinkDown = opLinkDown.read()\n",
    "                print(rdLinkDown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
